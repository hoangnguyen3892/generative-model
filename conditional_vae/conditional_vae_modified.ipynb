{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import importlib\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hoangnguyen/Documents/git/generative-model/conditional_vae'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hoangnguyen/Documents/git/generative-model/datasets\n"
     ]
    }
   ],
   "source": [
    "%cd ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mnist' from '/home/hoangnguyen/Documents/git/generative-model/datasets/mnist.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist; importlib.reload(mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- X: input\n",
    "- c: label\n",
    "- z: latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#or link to mnist.py\n",
    "#import mnist; importlib.reload(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = {\n",
    "    'batch_size': 1,\n",
    "    'no_cuda': True,\n",
    "    'epochs': 10,\n",
    "}\n",
    "args = argparse.Namespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mb_size = args.batch_size\n",
    "Z_dim = 100\n",
    "X_dim = 784\n",
    "y_dim = 10\n",
    "h_dim = 128\n",
    "cnt = 0\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist.MNIST('../data', train=True,\n",
    "                transform=transforms.ToTensor()), batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mnist.MNIST('../data', train=False,\n",
    "                transform=transforms.ToTensor()), batch_size=args.batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1608\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1961\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.2510  0.7529  0.6471  0.3333\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.2627  0.9373  0.9961  0.9961\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.1333  0.2235  0.7059\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.1059  0.1882  0.4039  0.5922\n",
      "  0.0000  0.0000  0.3137  0.3725  0.5255  0.9020  0.9961  0.9961  0.9961\n",
      "  0.7451  0.9608  0.9922  0.9961  0.9961  0.9765  0.9647  0.9961  0.9961\n",
      "  0.6706  0.8863  0.9961  0.7961  0.6706  0.6941  0.7569  0.9961  0.9961\n",
      "  0.0000  0.0549  0.0784  0.0314  0.0000  0.2706  0.9804  0.9961  0.8392\n",
      "  0.0000  0.0000  0.0000  0.0000  0.2314  0.9882  0.9961  0.9647  0.3294\n",
      "  0.0000  0.0000  0.0039  0.4000  0.9725  0.9961  0.9765  0.3216  0.0000\n",
      "  0.0000  0.0000  0.2902  0.9961  0.9961  0.8588  0.4275  0.2353  0.2471\n",
      "  0.0353  0.6980  0.9059  0.9961  0.9961  0.9059  0.9020  0.9882  0.9961\n",
      "  0.7216  0.9961  0.9961  0.9961  0.9961  0.8824  0.9490  0.8039  0.8549\n",
      "  0.8784  0.9961  0.9961  0.6745  0.2039  0.0863  0.1529  0.0118  0.3098\n",
      "  0.3765  0.3255  0.0196  0.0078  0.0000  0.0000  0.0000  0.0000  0.3098\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.6157\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.3373  0.9804\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2353  0.8431  0.9961\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.1490  0.8745  0.9961  0.9725\n",
      "  0.0000  0.0000  0.0000  0.0549  0.5922  0.9922  0.9961  0.8549  0.3412\n",
      "  0.0431  0.0431  0.3882  0.7294  0.9961  0.9961  0.8235  0.0902  0.0000\n",
      "  0.9961  0.9961  0.9961  0.9961  0.9961  0.5412  0.0902  0.0000  0.0000\n",
      "  0.9961  0.9961  0.6549  0.3412  0.1804  0.0039  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 18 to 26 \n",
      "   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.7608  0.4392  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9961  0.7333  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  1.0000  0.5059  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9373  0.0863  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2824  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2275  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9843  0.8392  0.1176  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9961  0.9961  0.1412  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9961  0.9961  0.1412  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9961  0.9961  0.1412  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9961  0.7961  0.0353  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9529  0.1765  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.9176  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.3412  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 27 to 27 \n",
      "   0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 1x1x28x28]\n",
      "\n",
      "\n",
      " 3\n",
      "[torch.LongTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = 0\n",
    "#DataLoader returns a batch size of X and y\n",
    "for X, c in train_loader:\n",
    "    print(X)\n",
    "    print(c)\n",
    "    label = c\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Initial weights randomly\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
    "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q(z|X): Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#X --> hidden layer: Initialize randomly\n",
    "Wxh = xavier_init(size=[X_dim + y_dim, h_dim])\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hidden --> z: Initialize randomly\n",
    "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_mu = Variable(torch.zeros(Z_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#hidden --> z: Initialize randomly\n",
    "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
    "bhz_var = Variable(torch.zeros(Z_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#@: support for python 3.5 - matmul\n",
    "#Q: encoder\n",
    "#Calculate z_mu and z_logvar\n",
    "def Q(X, c):\n",
    "    inputs = torch.cat([X, c], 1)\n",
    "    h = F.relu(inputs @ Wxh + bxh.repeat(inputs.size(0), 1))\n",
    "    #input.size(): 64x794 \n",
    "    #Wxh.size(): 794x128\n",
    "    #repeat(inputs.size(0), 1)): 64x1\n",
    "    #bxh.repeat: add bxh (1x128) at each row\n",
    "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
    "    z_logvar = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
    "    return z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_z(z_mu, z_logvar):\n",
    "    #eps = Variable(torch.randn(mb_size, Z_dim)) #randomize according to normal distribution\n",
    "    #The above can lead to bug because mb_size may be different from the sampling\n",
    "    eps = Variable(torch.randn(z_mu.size(0), Z_dim))\n",
    "    return z_mu + torch.exp(z_logvar / 2) * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# P(X|z): Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Wzh = xavier_init(size=[Z_dim + y_dim, h_dim])\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Whx = xavier_init(size=[h_dim, X_dim])\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def P(z, c):\n",
    "    inputs = torch.cat([z, c], 1)\n",
    "    h = F.relu(inputs @ Wzh + bzh.repeat(inputs.size(0), 1))\n",
    "    X = F.sigmoid(h @ Whx + bhx.repeat(h.size(0), 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def onehot(y):\n",
    "    y_array = np.zeros(shape=[y.size(0), 10], dtype='float32')\n",
    "    for index, ele in enumerate(y):\n",
    "        y_array[index][ele] = 1\n",
    "    return torch.from_numpy(y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
    "          Wzh, bzh, Whx, bhx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(params, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "        #flatten X: tensor 28x28 --> tensor 1x784\n",
    "        #one-hot c\n",
    "    cnt = 0\n",
    "    for it, (X, c) in enumerate(train_loader):\n",
    "        #If X is a numpy array, need to transform X from array to tensor to pass to Variable()\n",
    "        #X = Variable(torch.from_numpy(X))\n",
    "        \n",
    "        X = Variable(X.view(-1, 784))\n",
    "        c = Variable(onehot(c))\n",
    "        #c = Variable(torch.from_numpy(onehot(c).astype('float32')))\n",
    "        \n",
    "        #forward   \n",
    "        z_mu, z_logvar = Q(X, c)\n",
    "        z = sample_z(z_mu, z_logvar)\n",
    "        X_sample = P(z, c)\n",
    "        \n",
    "        #Loss\n",
    "        #recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
    "        #kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1))\n",
    "        \n",
    "        recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False)\n",
    "        kl_loss = torch.sum(0.5 * torch.sum(torch.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1))\n",
    "        loss = recon_loss + kl_loss\n",
    "       \n",
    "    \n",
    "        #Initialize zero buffers\n",
    "        for p in params:\n",
    "            p.grad.data.zero_()\n",
    "\n",
    "    \n",
    "        #Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print and plot every now and then\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter-{}; Loss: {:.4}'.format(it, loss.data[0]))\n",
    "\n",
    "            c = np.zeros(shape=[mb_size, y_dim], dtype='float32')\n",
    "            c[:, np.random.randint(0, 10)] = 1.\n",
    "            c = Variable(torch.from_numpy(c))\n",
    "            z = Variable(torch.randn(mb_size, Z_dim))\n",
    "            samples = P(z, c).data.numpy()[:16]\n",
    "\n",
    "            fig = plt.figure(figsize=(4, 4))\n",
    "            gs = gridspec.GridSpec(4, 4)\n",
    "            gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "            for i, sample in enumerate(samples):\n",
    "                ax = plt.subplot(gs[i])\n",
    "                plt.axis('off')\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_aspect('equal')\n",
    "                plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "            #if not os.path.exists('../conditional_vae/out/'):\n",
    "                #os.makedirs('../conditional_vae/out/')\n",
    "            \n",
    "            plt.savefig('../conditional_vae/out/{}.png'.format(str(cnt).zfill(3)), bbox_inches='tight') #to save\n",
    "            #plt.show()\n",
    "            cnt += 1\n",
    "            plt.close(fig)\n",
    "            \n",
    "            \n",
    "    #Print final loss of each epoch\n",
    "    #print('====> Epoch: {} Average loss: {:.4}'.format(epoch, loss.data[0] / len(train_loader.dataset)))\n",
    "    print('====> Epoch: {} Average loss: {:.4}'.format(epoch, loss.data[0] / args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hoangnguyen/Documents/git/generative-model/datasets'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0; Loss: 756.0\n",
      "Iter-1000; Loss: 187.2\n",
      "Iter-2000; Loss: 123.4\n",
      "Iter-3000; Loss: 160.7\n",
      "Iter-4000; Loss: 205.9\n",
      "Iter-5000; Loss: 170.5\n",
      "Iter-6000; Loss: 59.28\n",
      "Iter-7000; Loss: 137.6\n",
      "Iter-8000; Loss: 166.4\n",
      "Iter-9000; Loss: 186.4\n",
      "Iter-10000; Loss: 173.4\n",
      "Iter-11000; Loss: 105.8\n",
      "Iter-12000; Loss: 72.27\n",
      "Iter-13000; Loss: 175.1\n",
      "Iter-14000; Loss: 122.4\n",
      "Iter-15000; Loss: 127.6\n",
      "Iter-16000; Loss: 170.0\n",
      "Iter-17000; Loss: 166.9\n",
      "Iter-18000; Loss: 114.1\n",
      "Iter-19000; Loss: 91.59\n",
      "Iter-20000; Loss: 168.6\n",
      "Iter-21000; Loss: 123.2\n",
      "Iter-22000; Loss: 111.2\n",
      "Iter-23000; Loss: 130.6\n",
      "Iter-24000; Loss: 101.5\n",
      "Iter-25000; Loss: 215.8\n",
      "Iter-26000; Loss: 133.6\n",
      "Iter-27000; Loss: 186.0\n",
      "Iter-28000; Loss: 92.85\n",
      "Iter-29000; Loss: 153.7\n",
      "Iter-30000; Loss: 98.23\n",
      "Iter-31000; Loss: 149.0\n",
      "Iter-32000; Loss: 165.3\n",
      "Iter-33000; Loss: 178.9\n",
      "Iter-34000; Loss: 116.1\n",
      "Iter-35000; Loss: 183.5\n",
      "Iter-36000; Loss: 124.7\n",
      "Iter-37000; Loss: 163.2\n",
      "Iter-38000; Loss: 154.6\n",
      "Iter-39000; Loss: 99.27\n",
      "Iter-40000; Loss: 124.7\n",
      "Iter-41000; Loss: 114.5\n",
      "Iter-42000; Loss: 128.1\n",
      "Iter-43000; Loss: 167.1\n",
      "Iter-44000; Loss: 51.27\n",
      "Iter-45000; Loss: 224.9\n",
      "Iter-46000; Loss: 155.6\n",
      "Iter-47000; Loss: 126.3\n",
      "Iter-48000; Loss: 110.7\n",
      "Iter-49000; Loss: 135.0\n",
      "Iter-50000; Loss: 121.4\n",
      "Iter-51000; Loss: 183.1\n",
      "Iter-52000; Loss: 108.5\n",
      "Iter-53000; Loss: 197.6\n",
      "Iter-54000; Loss: 132.0\n",
      "Iter-55000; Loss: 97.29\n",
      "Iter-56000; Loss: 64.15\n",
      "Iter-57000; Loss: 186.3\n",
      "Iter-58000; Loss: 142.2\n",
      "Iter-59000; Loss: 176.4\n",
      "====> Epoch: 1 Average loss: 166.6\n",
      "Iter-0; Loss: 105.2\n",
      "Iter-1000; Loss: 140.6\n",
      "Iter-2000; Loss: 107.7\n",
      "Iter-3000; Loss: 116.0\n",
      "Iter-4000; Loss: 170.9\n",
      "Iter-5000; Loss: 129.0\n",
      "Iter-6000; Loss: 161.3\n",
      "Iter-7000; Loss: 103.5\n",
      "Iter-8000; Loss: 87.06\n",
      "Iter-9000; Loss: 138.4\n",
      "Iter-10000; Loss: 118.2\n",
      "Iter-11000; Loss: 84.16\n",
      "Iter-12000; Loss: 109.8\n",
      "Iter-13000; Loss: 197.6\n",
      "Iter-14000; Loss: 101.8\n",
      "Iter-15000; Loss: 142.0\n",
      "Iter-16000; Loss: 234.2\n",
      "Iter-17000; Loss: 127.0\n",
      "Iter-18000; Loss: 129.1\n",
      "Iter-19000; Loss: 70.35\n",
      "Iter-20000; Loss: 124.6\n",
      "Iter-21000; Loss: 112.2\n",
      "Iter-22000; Loss: 119.1\n",
      "Iter-23000; Loss: 160.0\n",
      "Iter-24000; Loss: 80.17\n",
      "Iter-25000; Loss: 39.72\n",
      "Iter-26000; Loss: 87.65\n",
      "Iter-27000; Loss: 136.7\n",
      "Iter-28000; Loss: 150.2\n",
      "Iter-29000; Loss: 137.3\n",
      "Iter-30000; Loss: 163.1\n",
      "Iter-31000; Loss: 152.4\n",
      "Iter-32000; Loss: 99.72\n",
      "Iter-33000; Loss: 180.3\n",
      "Iter-34000; Loss: 115.2\n",
      "Iter-35000; Loss: 104.2\n",
      "Iter-36000; Loss: 121.5\n",
      "Iter-37000; Loss: 140.5\n",
      "Iter-38000; Loss: 132.2\n",
      "Iter-39000; Loss: 129.9\n",
      "Iter-40000; Loss: 72.51\n",
      "Iter-41000; Loss: 119.4\n",
      "Iter-42000; Loss: 127.2\n",
      "Iter-43000; Loss: 113.9\n",
      "Iter-44000; Loss: 139.5\n",
      "Iter-45000; Loss: 148.7\n",
      "Iter-46000; Loss: 168.8\n",
      "Iter-47000; Loss: 131.3\n",
      "Iter-48000; Loss: 167.0\n",
      "Iter-49000; Loss: 138.2\n",
      "Iter-50000; Loss: 99.46\n",
      "Iter-51000; Loss: 155.6\n",
      "Iter-52000; Loss: 53.97\n",
      "Iter-53000; Loss: 86.64\n",
      "Iter-54000; Loss: 112.2\n",
      "Iter-55000; Loss: 156.1\n",
      "Iter-56000; Loss: 148.9\n",
      "Iter-57000; Loss: 119.4\n",
      "Iter-58000; Loss: 139.9\n",
      "Iter-59000; Loss: 157.7\n",
      "====> Epoch: 2 Average loss: 109.1\n",
      "Iter-0; Loss: 109.5\n",
      "Iter-1000; Loss: 102.8\n",
      "Iter-2000; Loss: 115.5\n",
      "Iter-3000; Loss: 187.1\n",
      "Iter-4000; Loss: 117.2\n",
      "Iter-5000; Loss: 170.9\n",
      "Iter-6000; Loss: 127.7\n",
      "Iter-7000; Loss: 145.9\n",
      "Iter-8000; Loss: 56.76\n",
      "Iter-9000; Loss: 82.91\n",
      "Iter-10000; Loss: 126.3\n",
      "Iter-11000; Loss: 150.6\n",
      "Iter-12000; Loss: 90.34\n",
      "Iter-13000; Loss: 170.0\n",
      "Iter-14000; Loss: 194.2\n",
      "Iter-15000; Loss: 112.5\n",
      "Iter-16000; Loss: 119.1\n",
      "Iter-17000; Loss: 45.87\n",
      "Iter-18000; Loss: 80.89\n",
      "Iter-19000; Loss: 122.8\n",
      "Iter-20000; Loss: 124.1\n",
      "Iter-21000; Loss: 149.4\n",
      "Iter-22000; Loss: 166.6\n",
      "Iter-23000; Loss: 106.9\n",
      "Iter-24000; Loss: 98.1\n",
      "Iter-25000; Loss: 64.37\n",
      "Iter-26000; Loss: 164.5\n",
      "Iter-27000; Loss: 101.0\n",
      "Iter-28000; Loss: 148.8\n",
      "Iter-29000; Loss: 124.4\n",
      "Iter-30000; Loss: 139.5\n",
      "Iter-31000; Loss: 112.7\n",
      "Iter-32000; Loss: 119.3\n",
      "Iter-33000; Loss: 95.07\n",
      "Iter-34000; Loss: 154.7\n",
      "Iter-35000; Loss: 84.8\n",
      "Iter-36000; Loss: 119.6\n",
      "Iter-37000; Loss: 115.8\n",
      "Iter-38000; Loss: 143.1\n",
      "Iter-39000; Loss: 118.8\n",
      "Iter-40000; Loss: 135.6\n",
      "Iter-41000; Loss: 86.26\n",
      "Iter-42000; Loss: 68.6\n",
      "Iter-43000; Loss: 139.0\n",
      "Iter-44000; Loss: 104.8\n",
      "Iter-45000; Loss: 135.8\n",
      "Iter-46000; Loss: 137.0\n",
      "Iter-47000; Loss: 130.3\n",
      "Iter-48000; Loss: 176.7\n",
      "Iter-49000; Loss: 83.39\n",
      "Iter-50000; Loss: 127.8\n",
      "Iter-51000; Loss: 115.7\n",
      "Iter-52000; Loss: 105.8\n",
      "Iter-53000; Loss: 92.64\n",
      "Iter-54000; Loss: 100.6\n",
      "Iter-55000; Loss: 129.6\n",
      "Iter-56000; Loss: 152.9\n",
      "Iter-57000; Loss: 87.09\n",
      "Iter-58000; Loss: 103.5\n",
      "Iter-59000; Loss: 163.6\n",
      "====> Epoch: 3 Average loss: 55.48\n",
      "Iter-0; Loss: 59.1\n",
      "Iter-1000; Loss: 106.7\n",
      "Iter-2000; Loss: 123.8\n",
      "Iter-3000; Loss: 130.3\n",
      "Iter-4000; Loss: 140.0\n",
      "Iter-5000; Loss: 100.1\n",
      "Iter-6000; Loss: 92.0\n",
      "Iter-7000; Loss: 48.9\n",
      "Iter-8000; Loss: 168.8\n",
      "Iter-9000; Loss: 93.68\n",
      "Iter-10000; Loss: 110.2\n",
      "Iter-11000; Loss: 45.07\n",
      "Iter-12000; Loss: 123.1\n",
      "Iter-13000; Loss: 130.3\n",
      "Iter-14000; Loss: 138.4\n",
      "Iter-15000; Loss: 100.4\n",
      "Iter-16000; Loss: 257.9\n",
      "Iter-17000; Loss: 81.5\n",
      "Iter-18000; Loss: 124.6\n",
      "Iter-19000; Loss: 83.12\n",
      "Iter-20000; Loss: 120.0\n",
      "Iter-21000; Loss: 175.9\n",
      "Iter-22000; Loss: 115.8\n",
      "Iter-23000; Loss: 93.56\n",
      "Iter-24000; Loss: 91.27\n",
      "Iter-25000; Loss: 159.6\n",
      "Iter-26000; Loss: 110.8\n",
      "Iter-27000; Loss: 113.2\n",
      "Iter-28000; Loss: 65.54\n",
      "Iter-29000; Loss: 122.9\n",
      "Iter-30000; Loss: 51.14\n",
      "Iter-31000; Loss: 88.16\n",
      "Iter-32000; Loss: 102.2\n",
      "Iter-33000; Loss: 135.0\n",
      "Iter-34000; Loss: 132.2\n",
      "Iter-35000; Loss: 99.33\n",
      "Iter-36000; Loss: 78.61\n",
      "Iter-37000; Loss: 175.7\n",
      "Iter-38000; Loss: 142.8\n",
      "Iter-39000; Loss: 90.27\n",
      "Iter-40000; Loss: 107.4\n",
      "Iter-41000; Loss: 132.8\n",
      "Iter-42000; Loss: 48.84\n",
      "Iter-43000; Loss: 124.6\n",
      "Iter-44000; Loss: 45.28\n",
      "Iter-45000; Loss: 125.1\n",
      "Iter-46000; Loss: 111.4\n",
      "Iter-47000; Loss: 104.4\n",
      "Iter-48000; Loss: 116.5\n",
      "Iter-49000; Loss: 104.3\n",
      "Iter-50000; Loss: 168.9\n",
      "Iter-51000; Loss: 129.9\n",
      "Iter-52000; Loss: 94.96\n",
      "Iter-53000; Loss: 49.18\n",
      "Iter-54000; Loss: 146.7\n",
      "Iter-55000; Loss: 78.33\n",
      "Iter-56000; Loss: 164.2\n",
      "Iter-57000; Loss: 131.6\n",
      "Iter-58000; Loss: 104.2\n",
      "Iter-59000; Loss: 118.5\n",
      "====> Epoch: 4 Average loss: 147.3\n",
      "Iter-0; Loss: 95.42\n",
      "Iter-1000; Loss: 42.46\n",
      "Iter-2000; Loss: 115.8\n",
      "Iter-3000; Loss: 94.92\n",
      "Iter-4000; Loss: 104.1\n",
      "Iter-5000; Loss: 143.0\n",
      "Iter-6000; Loss: 182.3\n",
      "Iter-7000; Loss: 112.4\n",
      "Iter-8000; Loss: 149.3\n",
      "Iter-9000; Loss: 106.8\n",
      "Iter-10000; Loss: 146.5\n",
      "Iter-11000; Loss: 103.9\n",
      "Iter-12000; Loss: 126.9\n",
      "Iter-13000; Loss: 192.1\n",
      "Iter-14000; Loss: 113.0\n",
      "Iter-15000; Loss: 162.5\n",
      "Iter-16000; Loss: 50.77\n",
      "Iter-17000; Loss: 140.3\n",
      "Iter-18000; Loss: 104.7\n",
      "Iter-19000; Loss: 133.7\n",
      "Iter-20000; Loss: 119.3\n",
      "Iter-21000; Loss: 148.9\n",
      "Iter-22000; Loss: 151.2\n",
      "Iter-23000; Loss: 96.49\n",
      "Iter-24000; Loss: 100.2\n",
      "Iter-25000; Loss: 214.7\n",
      "Iter-26000; Loss: 145.0\n",
      "Iter-27000; Loss: 59.24\n",
      "Iter-28000; Loss: 62.23\n",
      "Iter-29000; Loss: 113.3\n",
      "Iter-30000; Loss: 104.6\n",
      "Iter-31000; Loss: 93.0\n",
      "Iter-32000; Loss: 115.2\n",
      "Iter-33000; Loss: 167.6\n",
      "Iter-34000; Loss: 101.1\n",
      "Iter-35000; Loss: 130.9\n",
      "Iter-36000; Loss: 150.5\n",
      "Iter-37000; Loss: 87.79\n",
      "Iter-38000; Loss: 184.3\n",
      "Iter-39000; Loss: 119.9\n",
      "Iter-40000; Loss: 125.5\n",
      "Iter-41000; Loss: 94.25\n",
      "Iter-42000; Loss: 167.8\n",
      "Iter-43000; Loss: 138.3\n",
      "Iter-44000; Loss: 118.4\n",
      "Iter-45000; Loss: 151.3\n",
      "Iter-46000; Loss: 90.68\n",
      "Iter-47000; Loss: 102.8\n",
      "Iter-48000; Loss: 52.83\n",
      "Iter-49000; Loss: 117.6\n",
      "Iter-50000; Loss: 84.12\n",
      "Iter-51000; Loss: 139.9\n",
      "Iter-52000; Loss: 106.1\n",
      "Iter-53000; Loss: 148.3\n",
      "Iter-54000; Loss: 120.3\n",
      "Iter-55000; Loss: 169.0\n",
      "Iter-56000; Loss: 159.8\n",
      "Iter-57000; Loss: 75.68\n",
      "Iter-58000; Loss: 53.46\n",
      "Iter-59000; Loss: 107.9\n",
      "====> Epoch: 5 Average loss: 111.8\n",
      "Iter-0; Loss: 143.2\n",
      "Iter-1000; Loss: 129.0\n",
      "Iter-2000; Loss: 158.4\n",
      "Iter-3000; Loss: 82.52\n",
      "Iter-4000; Loss: 124.5\n",
      "Iter-5000; Loss: 96.76\n",
      "Iter-6000; Loss: 71.18\n",
      "Iter-7000; Loss: 48.54\n",
      "Iter-8000; Loss: 148.3\n",
      "Iter-9000; Loss: 121.2\n",
      "Iter-10000; Loss: 104.3\n",
      "Iter-11000; Loss: 74.57\n",
      "Iter-12000; Loss: 106.9\n",
      "Iter-13000; Loss: 172.1\n",
      "Iter-14000; Loss: 126.5\n",
      "Iter-15000; Loss: 70.64\n",
      "Iter-16000; Loss: 159.9\n",
      "Iter-17000; Loss: 89.82\n",
      "Iter-18000; Loss: 48.93\n",
      "Iter-19000; Loss: 93.57\n",
      "Iter-20000; Loss: 219.7\n",
      "Iter-21000; Loss: 128.1\n",
      "Iter-22000; Loss: 79.29\n",
      "Iter-23000; Loss: 130.8\n",
      "Iter-24000; Loss: 84.15\n",
      "Iter-25000; Loss: 110.2\n",
      "Iter-26000; Loss: 115.2\n",
      "Iter-27000; Loss: 151.2\n",
      "Iter-28000; Loss: 100.6\n",
      "Iter-29000; Loss: 104.7\n",
      "Iter-30000; Loss: 143.5\n",
      "Iter-31000; Loss: 167.5\n",
      "Iter-32000; Loss: 114.8\n",
      "Iter-33000; Loss: 46.13\n",
      "Iter-34000; Loss: 102.8\n",
      "Iter-35000; Loss: 86.33\n",
      "Iter-36000; Loss: 152.8\n",
      "Iter-37000; Loss: 113.7\n",
      "Iter-38000; Loss: 100.7\n",
      "Iter-39000; Loss: 127.6\n",
      "Iter-40000; Loss: 84.46\n",
      "Iter-41000; Loss: 70.02\n",
      "Iter-42000; Loss: 132.7\n",
      "Iter-43000; Loss: 155.3\n",
      "Iter-44000; Loss: 96.46\n",
      "Iter-45000; Loss: 135.3\n",
      "Iter-46000; Loss: 127.0\n",
      "Iter-47000; Loss: 100.5\n",
      "Iter-48000; Loss: 91.14\n",
      "Iter-49000; Loss: 203.9\n",
      "Iter-50000; Loss: 152.1\n",
      "Iter-51000; Loss: 108.8\n",
      "Iter-52000; Loss: 108.7\n",
      "Iter-53000; Loss: 138.7\n",
      "Iter-54000; Loss: 178.9\n",
      "Iter-55000; Loss: 101.0\n",
      "Iter-56000; Loss: 109.0\n",
      "Iter-57000; Loss: 122.7\n",
      "Iter-58000; Loss: 93.17\n",
      "Iter-59000; Loss: 114.7\n",
      "====> Epoch: 6 Average loss: 122.6\n",
      "Iter-0; Loss: 190.3\n",
      "Iter-1000; Loss: 163.0\n",
      "Iter-2000; Loss: 79.85\n",
      "Iter-3000; Loss: 125.6\n",
      "Iter-4000; Loss: 139.3\n",
      "Iter-5000; Loss: 138.5\n",
      "Iter-6000; Loss: 90.12\n",
      "Iter-7000; Loss: 107.0\n",
      "Iter-8000; Loss: 44.51\n",
      "Iter-9000; Loss: 82.02\n",
      "Iter-10000; Loss: 118.1\n",
      "Iter-11000; Loss: 150.5\n",
      "Iter-12000; Loss: 116.6\n",
      "Iter-13000; Loss: 161.8\n",
      "Iter-14000; Loss: 114.2\n",
      "Iter-15000; Loss: 128.2\n",
      "Iter-16000; Loss: 48.51\n",
      "Iter-17000; Loss: 61.64\n",
      "Iter-18000; Loss: 108.1\n",
      "Iter-19000; Loss: 43.21\n",
      "Iter-20000; Loss: 74.02\n",
      "Iter-21000; Loss: 169.3\n",
      "Iter-22000; Loss: 113.0\n",
      "Iter-23000; Loss: 156.5\n",
      "Iter-24000; Loss: 114.4\n",
      "Iter-25000; Loss: 107.5\n",
      "Iter-26000; Loss: 106.8\n",
      "Iter-27000; Loss: 184.7\n",
      "Iter-28000; Loss: 103.5\n",
      "Iter-29000; Loss: 135.9\n",
      "Iter-30000; Loss: 110.8\n",
      "Iter-31000; Loss: 136.9\n",
      "Iter-32000; Loss: 130.2\n",
      "Iter-33000; Loss: 114.8\n",
      "Iter-34000; Loss: 51.25\n",
      "Iter-35000; Loss: 148.5\n",
      "Iter-36000; Loss: 85.33\n",
      "Iter-37000; Loss: 127.2\n",
      "Iter-38000; Loss: 134.6\n",
      "Iter-39000; Loss: 103.8\n",
      "Iter-40000; Loss: 102.7\n",
      "Iter-41000; Loss: 97.45\n",
      "Iter-42000; Loss: 104.1\n",
      "Iter-43000; Loss: 111.3\n",
      "Iter-44000; Loss: 108.9\n",
      "Iter-45000; Loss: 102.2\n",
      "Iter-46000; Loss: 138.2\n",
      "Iter-47000; Loss: 179.9\n",
      "Iter-48000; Loss: 88.7\n",
      "Iter-49000; Loss: 140.0\n",
      "Iter-50000; Loss: 130.2\n",
      "Iter-51000; Loss: 131.6\n",
      "Iter-52000; Loss: 88.67\n",
      "Iter-53000; Loss: 127.9\n",
      "Iter-54000; Loss: 101.8\n",
      "Iter-55000; Loss: 184.6\n",
      "Iter-56000; Loss: 122.9\n",
      "Iter-57000; Loss: 113.4\n",
      "Iter-58000; Loss: 127.7\n",
      "Iter-59000; Loss: 128.1\n",
      "====> Epoch: 7 Average loss: 134.6\n",
      "Iter-0; Loss: 110.4\n",
      "Iter-1000; Loss: 142.7\n",
      "Iter-2000; Loss: 88.34\n",
      "Iter-3000; Loss: 139.1\n",
      "Iter-4000; Loss: 110.0\n",
      "Iter-5000; Loss: 106.8\n",
      "Iter-6000; Loss: 93.43\n",
      "Iter-7000; Loss: 108.8\n",
      "Iter-8000; Loss: 101.2\n",
      "Iter-9000; Loss: 119.1\n",
      "Iter-10000; Loss: 110.5\n",
      "Iter-11000; Loss: 191.0\n",
      "Iter-12000; Loss: 122.4\n",
      "Iter-13000; Loss: 112.5\n",
      "Iter-14000; Loss: 57.27\n",
      "Iter-15000; Loss: 95.66\n",
      "Iter-16000; Loss: 118.4\n",
      "Iter-17000; Loss: 91.83\n",
      "Iter-18000; Loss: 123.6\n",
      "Iter-19000; Loss: 119.9\n",
      "Iter-20000; Loss: 119.9\n",
      "Iter-21000; Loss: 91.53\n",
      "Iter-22000; Loss: 128.2\n",
      "Iter-23000; Loss: 157.2\n",
      "Iter-24000; Loss: 106.1\n",
      "Iter-25000; Loss: 146.2\n",
      "Iter-26000; Loss: 124.0\n",
      "Iter-27000; Loss: 106.1\n",
      "Iter-28000; Loss: 92.68\n",
      "Iter-29000; Loss: 86.89\n",
      "Iter-30000; Loss: 150.8\n",
      "Iter-31000; Loss: 122.8\n",
      "Iter-32000; Loss: 133.3\n",
      "Iter-33000; Loss: 46.29\n",
      "Iter-34000; Loss: 110.2\n",
      "Iter-35000; Loss: 140.7\n",
      "Iter-36000; Loss: 153.1\n",
      "Iter-37000; Loss: 39.77\n",
      "Iter-38000; Loss: 147.0\n",
      "Iter-39000; Loss: 99.52\n",
      "Iter-40000; Loss: 92.45\n",
      "Iter-41000; Loss: 53.14\n",
      "Iter-42000; Loss: 62.26\n",
      "Iter-43000; Loss: 101.4\n",
      "Iter-44000; Loss: 154.1\n",
      "Iter-45000; Loss: 129.1\n",
      "Iter-46000; Loss: 142.3\n",
      "Iter-47000; Loss: 115.3\n",
      "Iter-48000; Loss: 96.71\n",
      "Iter-49000; Loss: 122.9\n",
      "Iter-50000; Loss: 117.7\n",
      "Iter-51000; Loss: 125.0\n",
      "Iter-52000; Loss: 112.4\n",
      "Iter-53000; Loss: 92.67\n",
      "Iter-54000; Loss: 119.1\n",
      "Iter-55000; Loss: 139.4\n",
      "Iter-56000; Loss: 152.3\n",
      "Iter-57000; Loss: 93.7\n",
      "Iter-58000; Loss: 79.03\n",
      "Iter-59000; Loss: 131.9\n",
      "====> Epoch: 8 Average loss: 202.2\n",
      "Iter-0; Loss: 99.68\n",
      "Iter-1000; Loss: 117.0\n",
      "Iter-2000; Loss: 100.3\n",
      "Iter-3000; Loss: 112.2\n",
      "Iter-4000; Loss: 139.3\n",
      "Iter-5000; Loss: 83.63\n",
      "Iter-6000; Loss: 179.5\n",
      "Iter-7000; Loss: 140.0\n",
      "Iter-8000; Loss: 171.3\n",
      "Iter-9000; Loss: 171.8\n",
      "Iter-10000; Loss: 175.1\n",
      "Iter-11000; Loss: 164.1\n",
      "Iter-12000; Loss: 106.2\n",
      "Iter-13000; Loss: 103.5\n",
      "Iter-14000; Loss: 124.1\n",
      "Iter-15000; Loss: 202.3\n",
      "Iter-16000; Loss: 149.4\n",
      "Iter-17000; Loss: 170.0\n",
      "Iter-18000; Loss: 130.0\n",
      "Iter-19000; Loss: 107.1\n",
      "Iter-20000; Loss: 83.02\n",
      "Iter-21000; Loss: 89.94\n",
      "Iter-22000; Loss: 89.68\n",
      "Iter-23000; Loss: 109.2\n",
      "Iter-24000; Loss: 141.5\n",
      "Iter-25000; Loss: 88.17\n",
      "Iter-26000; Loss: 66.62\n",
      "Iter-27000; Loss: 137.4\n",
      "Iter-28000; Loss: 103.5\n",
      "Iter-29000; Loss: 174.8\n",
      "Iter-30000; Loss: 113.6\n",
      "Iter-31000; Loss: 124.2\n",
      "Iter-32000; Loss: 163.9\n",
      "Iter-33000; Loss: 105.1\n",
      "Iter-34000; Loss: 110.7\n",
      "Iter-35000; Loss: 118.0\n",
      "Iter-36000; Loss: 108.3\n",
      "Iter-37000; Loss: 120.4\n",
      "Iter-38000; Loss: 142.1\n",
      "Iter-39000; Loss: 136.7\n",
      "Iter-40000; Loss: 109.9\n",
      "Iter-41000; Loss: 134.2\n",
      "Iter-42000; Loss: 86.65\n",
      "Iter-43000; Loss: 93.54\n",
      "Iter-44000; Loss: 98.78\n",
      "Iter-45000; Loss: 136.0\n",
      "Iter-46000; Loss: 92.47\n",
      "Iter-47000; Loss: 118.6\n",
      "Iter-48000; Loss: 136.4\n",
      "Iter-49000; Loss: 124.3\n",
      "Iter-50000; Loss: 111.2\n",
      "Iter-51000; Loss: 110.3\n",
      "Iter-52000; Loss: 108.2\n",
      "Iter-53000; Loss: 97.57\n",
      "Iter-54000; Loss: 98.19\n",
      "Iter-55000; Loss: 51.37\n",
      "Iter-56000; Loss: 158.0\n",
      "Iter-57000; Loss: 142.3\n",
      "Iter-58000; Loss: 141.7\n",
      "Iter-59000; Loss: 129.6\n",
      "====> Epoch: 9 Average loss: 145.5\n",
      "Iter-0; Loss: 107.9\n",
      "Iter-1000; Loss: 139.5\n",
      "Iter-2000; Loss: 44.6\n",
      "Iter-3000; Loss: 74.13\n",
      "Iter-4000; Loss: 87.58\n",
      "Iter-5000; Loss: 84.28\n",
      "Iter-6000; Loss: 137.3\n",
      "Iter-7000; Loss: 135.3\n",
      "Iter-8000; Loss: 90.69\n",
      "Iter-9000; Loss: 65.03\n",
      "Iter-10000; Loss: 79.37\n",
      "Iter-11000; Loss: 114.7\n",
      "Iter-12000; Loss: 150.3\n",
      "Iter-13000; Loss: 39.71\n",
      "Iter-14000; Loss: 78.22\n",
      "Iter-15000; Loss: 55.39\n",
      "Iter-16000; Loss: 132.7\n",
      "Iter-17000; Loss: 128.1\n",
      "Iter-18000; Loss: 150.2\n",
      "Iter-19000; Loss: 139.7\n",
      "Iter-20000; Loss: 52.85\n",
      "Iter-21000; Loss: 114.6\n",
      "Iter-22000; Loss: 128.0\n",
      "Iter-23000; Loss: 164.1\n",
      "Iter-24000; Loss: 106.9\n",
      "Iter-25000; Loss: 82.56\n",
      "Iter-26000; Loss: 110.8\n",
      "Iter-27000; Loss: 96.28\n",
      "Iter-28000; Loss: 108.8\n",
      "Iter-29000; Loss: 130.8\n",
      "Iter-30000; Loss: 161.5\n",
      "Iter-31000; Loss: 136.2\n",
      "Iter-32000; Loss: 92.75\n",
      "Iter-33000; Loss: 95.27\n",
      "Iter-34000; Loss: 147.1\n",
      "Iter-35000; Loss: 110.7\n",
      "Iter-36000; Loss: 102.5\n",
      "Iter-37000; Loss: 146.4\n",
      "Iter-38000; Loss: 159.5\n",
      "Iter-39000; Loss: 139.8\n",
      "Iter-40000; Loss: 115.4\n",
      "Iter-41000; Loss: 136.9\n",
      "Iter-42000; Loss: 75.13\n",
      "Iter-43000; Loss: 133.5\n",
      "Iter-44000; Loss: 141.8\n",
      "Iter-45000; Loss: 118.8\n",
      "Iter-46000; Loss: 135.2\n",
      "Iter-47000; Loss: 128.3\n",
      "Iter-48000; Loss: 105.1\n",
      "Iter-49000; Loss: 116.4\n",
      "Iter-50000; Loss: 63.25\n",
      "Iter-51000; Loss: 138.0\n",
      "Iter-52000; Loss: 83.49\n",
      "Iter-53000; Loss: 146.3\n",
      "Iter-54000; Loss: 109.2\n",
      "Iter-55000; Loss: 96.69\n",
      "Iter-56000; Loss: 129.6\n",
      "Iter-57000; Loss: 137.0\n",
      "Iter-58000; Loss: 106.7\n",
      "Iter-59000; Loss: 140.1\n",
      "====> Epoch: 10 Average loss: 142.1\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
